Stable Diffusion Rebuilt from Scratch
*A modular, educational implementation of Stable Diffusion with custom UNet, VAE, CLIP encoder, and diffusion pipeline‚Äîusing pre-trained weights for accessibility.*


üìå Project Overview
This repository contains a from-scratch implementation of Stable Diffusion's core components (UNet, VAE encoder/decoder, CLIP text encoder, and DDPM sampling), with pre-trained weights loaded for practical inference. Ideal for:

üß† Learning how diffusion models work under the hood.

üîß Experimenting with custom architectures or training workflows.

üñºÔ∏è Generating images with optional pre-trained weights.

Key Distinction:

All model architectures are coded manually (PyTorch).

Pre-trained weights are loaded for quick inference (from Hugging Face diffusers).


üöÄ Features
Custom Implementations
UNet: Noise prediction network with time embeddings.

VAE: Variational Autoencoder (encoder/decoder) for latent space compression.

CLIP Text Encoder: Text embeddings for prompt conditioning (tokenizer from transformers).

DDPM Pipeline: Denoising and sampling logic (noise scheduling, predict_noise()).

Pre-Trained Components
Weights for UNet, VAE, and CLIP encoder loaded from sd-legacy/stable-diffusion-v1-5 via Hugging Face for demonstration.
